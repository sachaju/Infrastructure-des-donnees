{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91f3871",
   "metadata": {},
   "source": [
    "1: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a Lorem Ipsum of 3 paragraphs in a txt file using python, each paragraph delimited by two new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5164ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porro dolorem quisquam dolorem consectetur etincidunt. Numquam labore etincidunt tempora magnam quaerat. Etincidunt neque porro velit magnam dolor consectetur. Consectetur labore labore consectetur quiquia ut. Adipisci adipisci dolore non voluptatem. Voluptatem tempora dolor velit dolor. Velit quiquia quisquam voluptatem tempora dolorem. Numquam quaerat voluptatem sit ut dolorem. Ipsum quiquia modi aliquam numquam modi consectetur. Labore etincidunt numquam ut.\n",
      "\n",
      "Sit dolor amet modi modi etincidunt dolore. Non quisquam porro quisquam. Velit est ut magnam. Dolor tempora aliquam quaerat labore. Consectetur sit velit quaerat dolorem aliquam etincidunt sed. Amet dolor quaerat dolore sit modi. Ut etincidunt voluptatem dolor quiquia aliquam quiquia. Porro dolore dolor sit eius tempora porro. Est velit sit voluptatem. Magnam dolorem consectetur ut porro.\n",
      "\n",
      "Tempora dolore sed est dolor quiquia ut. Quisquam quiquia sed amet est tempora. Quiquia ipsum sed magnam. Ut ipsum adipisci adipisci dolor aliquam etincidunt. Quaerat adipisci sed non sit. Tempora dolore dolorem tempora. Voluptatem quaerat est neque. Tempora numquam eius ipsum voluptatem. Adipisci adipisci ipsum sed sit numquam sit. Dolor voluptatem sed magnam.\n",
      "\n",
      "lorem_paragraphs.txt\n"
     ]
    }
   ],
   "source": [
    "import lorem\n",
    "\n",
    "def generate_lorem_ipsum_paragraphs(num_paragraphs=3):\n",
    "    paragraphs = [lorem.paragraph() for _ in range(num_paragraphs)]\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = generate_lorem_ipsum_paragraphs(3)\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph)\n",
    "    print()\n",
    "def save_paragraphs_to_file(paragraphs, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for paragraph in paragraphs:\n",
    "            file.write(paragraph + '\\n\\n')\n",
    "\n",
    "paragraphs = generate_lorem_ipsum_paragraphs(3)\n",
    "filename = \"lorem_paragraphs.txt\"\n",
    "save_paragraphs_to_file(paragraphs, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9056b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aliquam sit etincidunt amet aliquam velit sed. Aliquam est voluptatem amet quiquia consectetur sit etincidunt. Sed dolore voluptatem non dolorem tempora sed tempora. Dolore velit ut magnam. Non numquam porro labore adipisci tempora aliquam. Tempora dolore non eius modi eius quiquia magnam. Amet porro adipisci quiquia quaerat. Quisquam quisquam amet neque dolor sit. Ipsum magnam neque neque tempora consectetur.\n",
      "\n",
      "Voluptatem adipisci labore adipisci quisquam dolorem quiquia. Est adipisci numquam quaerat quiquia. Dolor dolore etincidunt modi est. Numquam numquam aliquam sed. Voluptatem amet quisquam amet voluptatem velit neque neque. Ipsum modi magnam etincidunt ipsum amet eius neque. Adipisci labore voluptatem dolor consectetur ipsum.\n",
      "\n",
      "Dolor quisquam dolorem voluptatem amet quisquam. Porro magnam dolorem sed aliquam consectetur etincidunt. Sit sed adipisci sed ipsum non dolor amet. Dolore quiquia quisquam velit. Modi non labore dolore. Quisquam eius ipsum quaerat tempora quiquia. Dolorem voluptatem dolorem labore dolore sed. Porro amet non consectetur eius. Ut sit quaerat sit etincidunt. Etincidunt ipsum velit velit est.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"lorem_paragraphs.txt\", 'r') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94060b3a",
   "metadata": {},
   "source": [
    "2: Update the txt file by removing the first paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5d13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voluptatem adipisci labore adipisci quisquam dolorem quiquia. Est adipisci numquam quaerat quiquia. Dolor dolore etincidunt modi est. Numquam numquam aliquam sed. Voluptatem amet quisquam amet voluptatem velit neque neque. Ipsum modi magnam etincidunt ipsum amet eius neque. Adipisci labore voluptatem dolor consectetur ipsum.\n",
      "\n",
      "Dolor quisquam dolorem voluptatem amet quisquam. Porro magnam dolorem sed aliquam consectetur etincidunt. Sit sed adipisci sed ipsum non dolor amet. Dolore quiquia quisquam velit. Modi non labore dolore. Quisquam eius ipsum quaerat tempora quiquia. Dolorem voluptatem dolorem labore dolore sed. Porro amet non consectetur eius. Ut sit quaerat sit etincidunt. Etincidunt ipsum velit velit est.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"lorem_paragraphs.txt\"\n",
    "\n",
    "# Ouvrir le fichier en mode lecture\n",
    "with open(filename, 'r') as file:\n",
    "    # Lire les lignes du fichier\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Trouver l'index de la première ligne vide (fin du premier paragraphe)\n",
    "empty_line_index = lines.index('\\n')\n",
    "\n",
    "# Afficher le contenu sans le premier paragraphe\n",
    "for line in lines[empty_line_index+1:]:\n",
    "    print(line.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafdbd8",
   "metadata": {},
   "source": [
    "3: Create a dict from the paper of lecun et al. and goodfellow et al. with authors, title, affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d5e95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of LeCun's paper: Gradient-based learning applied to document recognition\n",
      "Authors of Goodfellow's paper: ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville']\n"
     ]
    }
   ],
   "source": [
    "papers_dict = {\n",
    "    \"LeCun_et_al\": {\n",
    "        \"title\": \"Gradient-based learning applied to document recognition\",\n",
    "        \"authors\": [\"Anna LeCun\", \"Bernard Boser\", \"Jeanne S. Denker\", \"Donnie Henderson\"],\n",
    "        \"affiliations\": [\"Harvard University, University of Strasbourg, Columbia University, Yale University\"]\n",
    "    },\n",
    "    \"Goodfellow_et_al\": {\n",
    "        \"title\": \"Generative Adversarial Nets\",\n",
    "        \"authors\": [\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "        \"affiliations\": [\"Montreal Institute for Learning Algorithms (MILA), University of Montreal, McGill University\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Accessing data example\n",
    "print(\"Title of LeCun's paper:\", papers_dict[\"LeCun_et_al\"][\"title\"])\n",
    "print(\"Authors of Goodfellow's paper:\", papers_dict[\"Goodfellow_et_al\"][\"authors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4026823",
   "metadata": {},
   "source": [
    "4: Save the previously created dict in the JSON format and load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd5c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun_et_al': {'title': 'Gradient-based learning applied to document recognition', 'authors': ['Anna LeCun', 'Bernard Boser', 'Jeanne S. Denker', 'Donnie Henderson'], 'affiliations': ['Harvard University, University of Strasbourg, Columbia University, Yale University']}, 'Goodfellow_et_al': {'title': 'Generative Adversarial Nets', 'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'affiliations': ['Montreal Institute for Learning Algorithms (MILA), University of Montreal, McGill University']}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"papers.json\", \"w\") as json_file:\n",
    "    json.dump(papers_dict, json_file, indent=4)\n",
    "\n",
    "with open(\"papers.json\", \"r\") as json_file:\n",
    "    loaded_papers_dict = json.load(json_file)\n",
    "\n",
    "print(loaded_papers_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea7b1c",
   "metadata": {},
   "source": [
    "5: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47c16b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "€\u0004•å\u0001\u0000\u0000\u0000\u0000\u0000\u0000}”(Œ\u000b",
      "LeCun_et_al”}”(Œ\u0005title”Œ7Gradient-based learning applied to document recognition”Œ\u0007authors”]”(Œ\n",
      "Anna LeCun”Œ\n",
      "Bernard Boser”Œ\u0010Jeanne S. Denker”Œ\u0010Donnie Henderson”eŒ\f",
      "affiliations”]”ŒRHarvard University, University of Strasbourg, Columbia University, Yale University”auŒ\u0010Goodfellow_et_al”}”(h\u0003Œ\u001bGenerative Adversarial Nets”h\u0005]”(Œ\u000eIan Goodfellow”Œ\n",
      "Yoshua Bengio”Œ\u000fAaron Courville”eh\u000b",
      "]”Œ\\Montreal Institute for Learning Algorithms (MILA), University of Montreal, McGill University”auu.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open(\"papers.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(papers_dict, pickle_file)\n",
    "with open('papers.pickle', 'r', errors='ignore') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0563b66",
   "metadata": {},
   "source": [
    "6: Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea92b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree\n",
    "\n",
    "xml_file = \"C:/Users/sacha/Downloads/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)\n",
    "# Prettify = Make it human readable\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be042649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2015-09-01',\n",
       " 'hour': '08:30',\n",
       " 'to': 'Tove',\n",
       " 'from': 'Jani',\n",
       " 'body': \"Don't forget me this weekend!\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import lxml.etree\n",
    "\n",
    "# Path to the XML file\n",
    "xml_file = \"C:/Users/sacha/Downloads/xml_file2.nxml\"\n",
    "\n",
    "# Parse the XML file\n",
    "root = lxml.etree.parse(xml_file)\n",
    "\n",
    "# Extract information and store it in a dictionary\n",
    "xml_data = {}\n",
    "for element in root.iter():\n",
    "    if element.tag == \"note\":\n",
    "        xml_data[\"date\"] = element.find(\"date\").text\n",
    "        xml_data[\"hour\"] = element.find(\"hour\").text\n",
    "        xml_data[\"to\"] = element.find(\"to\").text\n",
    "        xml_data[\"from\"] = element.find(\"from\").text\n",
    "        xml_data[\"body\"] = element.find(\"body\").text\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "json_file = \"xml_data.json\"\n",
    "with open(json_file, \"w\") as file:\n",
    "    json.dump(xml_data, file)\n",
    "\n",
    "xml_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7124f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\sacha\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n",
      "{'note': {'date': '2015-09-01', 'hour': '08:30', 'to': 'Tove', 'from': 'Jani', 'body': \"Don't forget me this weekend!\"}}\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "xml_file = \"C:/Users/sacha/Downloads/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True)) \n",
    "with open(xml_file, \"rb\") as f:\n",
    "    xml_dict = xmltodict.parse(f)\n",
    "print(xml_dict)\n",
    "# Convert the dictionary to JSON\n",
    "json_data = json.dumps(xml_dict, indent=4)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"xml_data.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47e5c7",
   "metadata": {},
   "source": [
    "7: Download an image of your choice and save it in either jpg or png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3ae31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "im = Image.open(\"C:/Users/sacha/Downloads/chutesdekarera.jpg\")\n",
    "im.save(\"C:/Users/sacha/Downloads/chutesdekarera.jpg\", \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc4b5b",
   "metadata": {},
   "source": [
    "8: From the data/Chap2/data_world.json file, create a set of publisher type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d353046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'org:Organization'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file\n",
    "with open(\"C:/Users/sacha/Downloads/data_world.json\") as file:\n",
    "    # Load JSON data\n",
    "    json_data = json.load(file)\n",
    "\n",
    "    # Initialize an empty set to store publisher types\n",
    "    publisher_types = set()\n",
    "\n",
    "    # Iterate over each item in the JSON array\n",
    "    for item in json_data:\n",
    "        # Extract the \"publisher\" object from the item\n",
    "        publisher = item.get('publisher')\n",
    "\n",
    "        # If publisher exists and it has \"@type\" key\n",
    "        if publisher and '@type' in publisher:\n",
    "            # Add the \"@type\" value to the set\n",
    "            publisher_types.add(publisher['@type'])\n",
    "\n",
    "# Now publisher_types set contains unique publisher types\n",
    "print(publisher_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b95493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'org:Organization'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file\n",
    "with open(\"C:/Users/sacha/Downloads/data_world.json\") as file:\n",
    "    # Load JSON data\n",
    "    json_data = json.load(file)\n",
    "\n",
    "    # Initialize an empty set to store publisher types\n",
    "    publisher_types = set()\n",
    "\n",
    "    # Iterate over each item in the JSON array\n",
    "    for item in json_data:\n",
    "        # Extract the \"publisher\" object from the item\n",
    "        publisher = item.get('publisher')\n",
    "\n",
    "        # If publisher exists and it has \"@type\" key\n",
    "        if publisher and '@type' in publisher:\n",
    "            # Extract the \"@type\" value and add it to the set\n",
    "            publisher_types.add(publisher['@type'])\n",
    "\n",
    "# Now publisher_types set contains unique publisher types\n",
    "print(publisher_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fe24a",
   "metadata": {},
   "source": [
    "9: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab3fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 'spatial' deleted and data saved to data_world_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON data from data_world.json\n",
    "with open(\"C:/Users/sacha/Downloads/data_world.json\") as file:\n",
    "    data_world = json.load(file)\n",
    "\n",
    "# Delete the key of your choice\n",
    "key_to_delete = \"spatial\"\n",
    "if key_to_delete in data_world:\n",
    "    del data_world[key_to_delete]\n",
    "\n",
    "# Write the modified dictionary to data_world_cleaned.json\n",
    "with open('data_world_cleaned.json', 'w') as file:\n",
    "    json.dump(data_world, file, indent=4)\n",
    "\n",
    "print(\"Key '{}' deleted and data saved to data_world_cleaned.json\".format(key_to_delete))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb77d58",
   "metadata": {},
   "source": [
    "10: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90711bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrix:\n",
      "Access Level: public\n",
      "\tAccrual Periodicity: irregular - Count: 4961\n",
      "\tAccrual Periodicity: R/P1D - Count: 5\n",
      "\tAccrual Periodicity: R/P1M - Count: 3\n",
      "\tAccrual Periodicity: R/PT1S - Count: 1\n",
      "\tAccrual Periodicity: R/P3M - Count: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON data from data_world.json\n",
    "with open(\"C:/Users/sacha/Downloads/data_world.json\") as file:\n",
    "    data_world = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the co-occurrence counts\n",
    "co_occurrence_matrix = {}\n",
    "\n",
    "# Iterate over each item in the data\n",
    "for item in data_world:\n",
    "    access_level = item.get('accessLevel')\n",
    "    accrual_periodicity = item.get('accrualPeriodicity')\n",
    "\n",
    "    # Increment the count for the combination of accessLevel and accrualPeriodicity\n",
    "    if access_level and accrual_periodicity:\n",
    "        if access_level not in co_occurrence_matrix:\n",
    "            co_occurrence_matrix[access_level] = {}\n",
    "        if accrual_periodicity not in co_occurrence_matrix[access_level]:\n",
    "            co_occurrence_matrix[access_level][accrual_periodicity] = 0\n",
    "        co_occurrence_matrix[access_level][accrual_periodicity] += 1\n",
    "\n",
    "# Print the co-occurrence matrix\n",
    "print(\"Co-occurrence Matrix:\")\n",
    "for access_level, accrual_counts in co_occurrence_matrix.items():\n",
    "    print(f\"Access Level: {access_level}\")\n",
    "    for accrual_periodicity, count in accrual_counts.items():\n",
    "        print(f\"\\tAccrual Periodicity: {accrual_periodicity} - Count: {count}\")\n",
    "\n",
    "# You can also write this matrix to a file if needed\n",
    "# with open('co_occurrence_matrix.json', 'w') as matrix_file:\n",
    "#     json.dump(co_occurrence_matrix, matrix_file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
